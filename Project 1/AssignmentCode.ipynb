{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac797774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create representative sampling -> proportional to each month\n",
    "# TODO: Visualize weekly each \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-color",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "## Exploratory data analysis and preprocessing\n",
    "\n",
    "The **exploratory data analysis** is the statistical treatment to which the samples collected during a research process in any scientific field are subjected.\n",
    "For greater speed and accuracy, the entire process is usually carried out by computer, with specific applications for statistical treatment.\n",
    "\n",
    "### Application to Data Mining\n",
    "\n",
    "In **data mining**, although not mandatory, it is a good practice to analyze the data you will be working with in order to observe its main characteristics in order to get an idea of the structure of the data set, and identify the target variable and possible modeling techniques.\n",
    "\n",
    "**Basic Process**\n",
    "- *Transform the data*: It helps us know what to do with null, missing values, or atypical data. In addition, it establishes if there is a need to reduce the dimensionality of data.\n",
    "- *Visualize*: Use some tool to make a graphical representation of the data, for example, R, Jupyter notebook, Google Colab, etc.\n",
    "- *Analyze and interpret*: Analyze and interpret the data through different visualizations.\n",
    "- *Document*: Document all the graphs and statistics generated.\n",
    "\n",
    "This process is also helpful when reviewing the data description to understand the meaning of each characteristic.\n",
    "\n",
    "There are several activities in doing an exploratory data analysis but in terms of data mining the key points to be made are:\n",
    "\n",
    "- Description of the data structure.\n",
    "- Identification of missing data.\n",
    "- Detection of outliers.\n",
    "- Identification of relationships between variable pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-africa",
   "metadata": {},
   "source": [
    "The **goal of this project** is to learn how to do data exploration. In this case, data from **New York City Yellow Taxis** is used.\n",
    "\n",
    "At the end of the notebook, you should be able to answer the following question:\n",
    "\n",
    "\n",
    "## How has covid affected the use of taxis in New York?\n",
    "\n",
    "Some of the questions you will ask yourselves throughout the notebook are:\n",
    "- How has covid changed the use of taxis in NYC?\n",
    "- What pick-up distribution do the taxis follow and what distance / duration do they take?\n",
    "- What are the areas where taxis are picked up the least? And where else do people go?\n",
    "- What are the most usual times?\n",
    "- Which days of the week and month are used the most? Possible reasons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-helena",
   "metadata": {},
   "source": [
    "**Install and import the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b1c45c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (10.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from pyarrow) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyshp in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (2.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shapely in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (1.8.5.post1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: descartes in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from descartes) (3.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (1.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (9.3.0)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (1.23.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib->descartes) (4.38.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->descartes) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\msi\\documents\\trasladar\\yo\\software engineering\\data science & machine learning\\0. course\\project 1\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow\n",
    "! pip install pyshp\n",
    "! pip install shapely\n",
    "! pip install descartes\n",
    "! pip install pandas matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "express-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "from shapely.geometry import Polygon\n",
    "from descartes.patch import PolygonPatch\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "YEARS = [2019, 2020, 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-sight",
   "metadata": {},
   "source": [
    "First of all, you need to download the data:\n",
    "\n",
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fantastic-footage",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c60127ac9f6402fa6c0d885a41c801e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the Trip Record Data\n",
    "for year in tqdm(YEARS):\n",
    "    if not os.path.exists(f'data/{year}'):\n",
    "        os.makedirs(f'data/{year}', exist_ok=True)\n",
    "        for month in tqdm(range(1, 13)): \n",
    "            urllib.request.urlretrieve(f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02d}.parquet', f'data/{year}/{month:02d}.parquet')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-values",
   "metadata": {},
   "source": [
    "## 01. Data cleaning\n",
    "\n",
    "In order to have clean and useful data, it is necessary to delete all those rows that contain corrupt information:\n",
    "- The pick-up is after the drop-off.\n",
    "- Dates are imported by months and years. Are the dates correct?\n",
    "- Traveling with zero passengers?\n",
    "- Do you travel very long or particularly short?\n",
    "- Negative payments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-collapse",
   "metadata": {},
   "source": [
    "**Data Dictionary**\n",
    "\n",
    "Only the next columns are needed:\n",
    "\n",
    "- *tpep_pickup_datetime*: The date and time when the meter was engaged.\n",
    "- *tpep_dropoff_datetime*: The date and time when the meter was disengaged. \n",
    "- *Passenger_count*: The number of passengers in the vehicle. (This is a driver-entered value)\n",
    "- *Trip_distance*: The elapsed trip distance in miles reported by the taximeter.\n",
    "- *PULocationID*: TLC Taxi Zone in which the taximeter was engaged\n",
    "- *DOLocationID*: TLC Taxi Zone in which the taximeter was disengaged\n",
    "- *Payment_type*: A numeric code signifying how the passenger paid for the trip. \n",
    "    - 1= Credit card\n",
    "    - 2= Cash\n",
    "    - 3= No charge\n",
    "    - 4= Dispute\n",
    "    - 5= Unknown\n",
    "    - 6= Voided trip\n",
    "- *Fare_amount*: The time-and-distance fare calculated by the meter.\n",
    "- *Total_amount*: The total amount charged to passengers. Does not include cash tips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-bloom",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "\n",
    "- To speed up the calculations and reduce the computation time, do a uniform sampling of the data (a sample out of 1000).\n",
    "- Datetime columns are *to_datetime* series (help to search functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dependent-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(year: str, month: str):\n",
    "    \"\"\"\n",
    "    Function that reads the downloaded data and converts it to a DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    return pq.read_table(f'data/{year}/{str(month).zfill(2)}.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c291d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2019'\n",
    "month = '01'\n",
    "\n",
    "df = load_table(year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67282652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:46:40</td>\n",
       "      <td>2019-01-01 00:53:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>151</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:59:47</td>\n",
       "      <td>2019-01-01 01:18:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-21 13:48:30</td>\n",
       "      <td>2018-12-21 13:52:40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-11-28 15:52:25</td>\n",
       "      <td>2018-11-28 15:55:45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-11-28 15:56:57</td>\n",
       "      <td>2018-11-28 15:58:33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>55.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2019-01-01 00:46:40   2019-01-01 00:53:20              1.0   \n",
       "1         1  2019-01-01 00:59:47   2019-01-01 01:18:59              1.0   \n",
       "2         2  2018-12-21 13:48:30   2018-12-21 13:52:40              3.0   \n",
       "3         2  2018-11-28 15:52:25   2018-11-28 15:55:45              5.0   \n",
       "4         2  2018-11-28 15:56:57   2018-11-28 15:58:33              5.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            1.5         1.0                  N           151           239   \n",
       "1            2.6         1.0                  N           239           246   \n",
       "2            0.0         1.0                  N           236           236   \n",
       "3            0.0         1.0                  N           193           193   \n",
       "4            0.0         2.0                  N           193           193   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1          7.0    0.5      0.5        1.65           0.0   \n",
       "1             1         14.0    0.5      0.5        1.00           0.0   \n",
       "2             1          4.5    0.5      0.5        0.00           0.0   \n",
       "3             2          3.5    0.5      0.5        0.00           0.0   \n",
       "4             2         52.0    0.0      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge airport_fee  \n",
       "0                    0.3          9.95                   NaN        None  \n",
       "1                    0.3         16.30                   NaN        None  \n",
       "2                    0.3          5.80                   NaN        None  \n",
       "3                    0.3          7.55                   NaN        None  \n",
       "4                    0.3         55.55                   NaN        None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31705706",
   "metadata": {},
   "source": [
    "We will develop **OOP** classes to make the cleaning of the data, so we can **encapsulate** the funtionalities, and in the case we wanted, would be very easy for us to keep track of how the data is changing in each step.\n",
    "\n",
    "In each object, we have the one or more **atomical functions** (*static methods*). For those, there are simple **tests** to make sure that they are working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5800d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class BoundaryFilters():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_nan(\n",
    "        self,\n",
    "        column\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Remove the NaN values that we have\n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = self.df[np.logical_not(np.isnan(self.df[column]))]\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_lower_bound_field(\n",
    "        self,\n",
    "        column: str, \n",
    "        value: float = 0.0,\n",
    "        convert_int: bool = False\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Filter the values which are lower than a certain value for a given column.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - column: the column for which we want to filter the values\n",
    "        - value: the value that we  want to set lower bound\n",
    "        - convert_int: whether we want to convert the variable into an integer\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe the field lower bounded\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # First we remove the NaNs, we we would not be able to make this filter\n",
    "        self.remove_nan(self, column)\n",
    "\n",
    "        if convert_int:\n",
    "            self.df[column] = self.df[column].astype(int)\n",
    "        self.df = self.df[self.df[column] > value]\n",
    "        \n",
    "        # We make the text\n",
    "        assert min(self.df[column]) > value\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    @staticmethod\n",
    "    def set_upper_bound_field(\n",
    "        self,\n",
    "        column: str,\n",
    "        value: float,\n",
    "        convert_int: bool = False\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Filter the values which are higher than a certain value for a given column.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - column: the column for which we want to filter the values\n",
    "        - value: the value that we  want to set higher bound\n",
    "        - convert_int: whether we want to convert the variable into an integer\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe the field higher bounded\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # First we remove the NaNs, we we would not be able to make this filter\n",
    "        self.remove_nan(self, column)\n",
    "        \n",
    "        if convert_int:\n",
    "            self.df[column] = self.df[column].astype(int)\n",
    "        self.df = self.df[self.df[column] < value]\n",
    "        \n",
    "        # We make the text\n",
    "        assert max(self.df[column]) <= value\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def filter_lower_bound(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Apply to a group of fields a lower bound.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with a lower bound applied to a filter\n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "        self.set_lower_bound_field(self, 'passenger_count', True)\n",
    "        self.set_lower_bound_field(self, 'total_amount')\n",
    "        self.set_lower_bound_field(self, 'fare_amount')\n",
    "        self.set_lower_bound_field(self, 'trip_distance')\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def filter_upper_bound(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Apply to a group of fields a higher bound.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with a higher bound applied to a filter\n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "        self.set_upper_bound_field(self, 'trip_distance', 6700)\n",
    "        self.set_upper_bound_field(self, 'payment_type', 7)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def filter_positive_diff(\n",
    "        self,\n",
    "        fields: List[str] = ['tpep_dropoff_datetime', 'tpep_pickup_datetime'],\n",
    "        is_date: bool = True\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to filter for the voyages that had a duration which is > 0.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - fields: the fields for which we will make a comparison and filter them by positivity\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with only voyages that had a duration > 0\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.df['diff'] = self.df[fields[0]] - self.df[fields[1]]\n",
    "        \n",
    "        if is_date:\n",
    "            self.df['diff'] = self.df['diff'].dt.total_seconds()\n",
    "\n",
    "        # Filter by positive voyages\n",
    "        self.set_lower_bound_field(self, 'diff')\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def apply_filters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to apply a group of filters into the dataframe.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with the filters applied to it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.filter_lower_bound()\n",
    "        self.filter_upper_bound()\n",
    "        self.filter_positive_diff()\n",
    "        \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93974769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckField():\n",
    "    def __init__(self, df, year, month):\n",
    "        self.df = df\n",
    "        self.year = str(year).strip()\n",
    "        self.month = str(month).strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def correct_field(\n",
    "        self,\n",
    "        field_name: str,\n",
    "        field_value: float,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to check that a certain field has consistent values with its context (correct year, month, ...)\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - field_name: the field for which we want to check for consistent values\n",
    "        - field_value: the value for which the field is consistent with its context\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with the field consistent to its value\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if field_name not in ['year', 'month']:\n",
    "            raise Exception('Field Name must be: [year, month]')\n",
    "            \n",
    "        self.df[field_name] = self.df[field_name].apply(lambda x: str(x))\n",
    "        self.df = self.df[self.df[field_name] == field_value]\n",
    "        \n",
    "        # And we need to make sure that this field now only has our value wanted\n",
    "        assert self.df[field_name].unique() == field_value\n",
    "\n",
    "        return self.df\n",
    "        \n",
    "    def apply_correctness_field(\n",
    "        self,\n",
    "        field: str,\n",
    "        only_year: bool = False\n",
    "    ):        \n",
    "        \n",
    "        \"\"\"\n",
    "        Function to apply correctness about the year and month values on a certain field.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - field: the field for which we want to apply correcntess on the month & year\n",
    "        - only_year: if we want to apply correctness of only year\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with the field consistent to the year & month context\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Add the data needed\n",
    "        self.df['year'] = self.df[field].dt.year\n",
    "        self.df['month'] = self.df[field].dt.month\n",
    "        \n",
    "        # Simplify the data: both pickup and dropoff on the same month, year & day\n",
    "        self.correct_field(self, 'year', self.year)\n",
    "        \n",
    "        if not only_year:\n",
    "            self.correct_field(self, 'month', self.month)\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def apply_correctness(\n",
    "        self\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Apply correcntess of the month & year on a group of fields.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with the fields consistent to the year & month context\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply the correctness\n",
    "        self.apply_correctness_field('tpep_pickup_datetime')\n",
    "        self.apply_correctness_field('tpep_dropoff_datetime')\n",
    "        \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b57b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class CleanData(BoundaryFilters, CheckField):\n",
    "    def __init__(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        year: str, \n",
    "        month: str\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialization of the Clean Data object, which have two main functionalities: Limit Boundaries & Check Fields\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - df: dataframe that we want to clean\n",
    "        - year: the year in which the dataframe has data\n",
    "        - month: the month in which the dataframe has data\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = df\n",
    "        self.year = str(year).strip()\n",
    "        self.month = str(month).strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def pre_process(\n",
    "        self,\n",
    "        columns: List[str],\n",
    "        columns_filter: bool = True,\n",
    "        rows_filter: bool = True,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Pre Process the dataframe, which means:\n",
    "        - Only consider the columns we want\n",
    "        - Dropping duplicate rows\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - columns: the columns we want to consider only in the df\n",
    "        - columns_filter: if we want to apply the filter on the columns\n",
    "        - rows_filter: if we want to apply the filter on the rows\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with the filters applied to it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Filtering the columns we have in the df\n",
    "        if columns_filter:\n",
    "            self.df = self.df[columns]\n",
    "            \n",
    "        # Filtering the rows we have in the df\n",
    "        if rows_filter:\n",
    "            self.df.drop_duplicates(inplace=True, keep='first', ignore_index=True)\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample(\n",
    "        self,\n",
    "        n\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to create a sample on the dataframe.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - n: the amount we want to reduce the initial dataframe\n",
    "        \n",
    "        Returns:\n",
    "        - df: the dataframe which is only a sample of the initial one\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        previous_length = len(self.df)\n",
    "        \n",
    "        # Create the dataframe to be a fraction of the inital one\n",
    "        frac = 1/n\n",
    "        self.df = self.df.sample(frac=frac)\n",
    "        \n",
    "        # We have to make sure that the size is correct (with a certain error)\n",
    "        # TODO: Create a function to apply this assertion\n",
    "        # assert len(self.df) * n - previous_length < previous_length / n ** 1.4\n",
    "        print(len(self.df), previous_length)\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def clean_data(\n",
    "        self, \n",
    "        columns: List[str],\n",
    "        n: int = 1000\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function to apply the cleaning of the data on a dataframe.\n",
    "        \n",
    "        Params:\n",
    "        - self\n",
    "        - columns: list of columns that we want only to consider in the analysis\n",
    "        - n: the amount by which we want to reduce the initial dataframe\n",
    "        \n",
    "        Returns:\n",
    "        - df: dataframe with the data in it cleaned\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample first to speed up the process (not recommended, but just for learning pursposes)\n",
    "        self.sample(self, n)\n",
    "\n",
    "        self.pre_process(self, columns)\n",
    "        self.apply_filters()\n",
    "        self.apply_correctness()\n",
    "        \n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caring-timber",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "required_data = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', \n",
    "                 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'total_amount']\n",
    "\n",
    "def clean_data(data, year, month, sampling = 1000):\n",
    "    \"\"\"\n",
    "    Function that clears the month data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using Objects we can encapsulate the functionalities we want (and we could keep track of each state of the process)\n",
    "    clean_data_instance = CleanData(data, year, month)\n",
    "    \n",
    "    clean_data_df = clean_data_instance.clean_data(\n",
    "        columns=required_data,\n",
    "        n=sampling\n",
    "    )\n",
    "    \n",
    "    return clean_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-estimate",
   "metadata": {},
   "source": [
    "In the ***post_processing*** function you can add all information you need in order to perform the necessary exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "322e21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "class PostProcessing():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def set_outliers_column(\n",
    "        self,\n",
    "        low: float,\n",
    "        high: float,\n",
    "        column: str,\n",
    "        output_column_name: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We are going to build a new column with the information about the outlier.\n",
    "        \n",
    "        Params:\n",
    "        - low: which is the value of the lower outlier\n",
    "        - high: which is the value of the high outlier\n",
    "        - column: for which column do we want to set the outlier\n",
    "        - output_column_name: which is the name of the column with the information whether is an outlier\n",
    "        \"\"\"\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            if row[column] < low:\n",
    "                self.df.at[idx, output_column_name] = -1\n",
    "            \n",
    "            if row[column] > high:\n",
    "                self.df.at[idx, output_column_name] = 1\n",
    "            \n",
    "            else:\n",
    "                self.df.at[idx, output_column_name] = 0\n",
    "        \n",
    "        # By default would put a float type, we can change it to int\n",
    "        self.df = self.df.astype({output_column_name: int})\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def detect_outliers_field(\n",
    "        self,\n",
    "        field: str,\n",
    "        output_column_name: str,\n",
    "        lower: float = 0.01,\n",
    "        higher: float = 0.99,\n",
    "        is_time: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Function to detect the high and lower quantiles.\n",
    "        \n",
    "        Params:\n",
    "        - field: for which field do we compute quantiles\n",
    "        - output_column_name: which is the name of the column with the information whether is an outlier\n",
    "        - lower: which lower quantile is going to be the boundary for lower outliers\n",
    "        - higher: which higher quantile is going to be the boundary for higher outliers\n",
    "        - is_time: whether the column is a time or not\n",
    "        \"\"\"\n",
    "        \n",
    "        low, high = self.df[field].quantile([lower, higher])\n",
    "        \n",
    "         # We set the percentiles in a new oclumn\n",
    "        self.set_outliers_column(\n",
    "            self,\n",
    "            low,\n",
    "            high,\n",
    "            column=field,\n",
    "            output_column_name=output_column_name,\n",
    "        )\n",
    "                \n",
    "        # Clean the changes we have made\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    def detect_outliers_diff_fields(\n",
    "        self,\n",
    "        fields: List[str] = ['tpep_dropoff_datetime', 'tpep_pickup_datetime'],\n",
    "        output_column_name: str = 'voyage_time_outliers',\n",
    "        lower: float = 0.01,\n",
    "        higher: float = 0.99,\n",
    "        is_time: bool = False,\n",
    "        keep_column: bool = False\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Detect which are the percentiles in a difference between two fields, and filter them.\n",
    "        \n",
    "        Params:\n",
    "        - fields: which are the fields that we want to make the difference\n",
    "        - output_column: the column that results of the detection of the percentile\n",
    "        - lower: the lower percentile we want to detect\n",
    "        - higher: the higher percentile we want to detect\n",
    "        - is_time: if the fields are dates or datetimes\n",
    "        - keep_column: if we want to keep the column of diff\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.df['diff'] = self.df[fields[0]] - self.df[fields[1]]\n",
    "        \n",
    "        if is_time:\n",
    "            self.df['diff'] = self.df['diff'].dt.total_seconds()\n",
    "\n",
    "        low, high = self.df['diff'].quantile([lower, higher])\n",
    "        \n",
    "        # We set the percentiles in a new oclumn\n",
    "        self.set_outliers_column(\n",
    "            self,\n",
    "            low,\n",
    "            high,\n",
    "            column='diff',\n",
    "            output_column_name=output_column_name,\n",
    "        )\n",
    "                \n",
    "        # Clean the changes we have made\n",
    "        if not keep_column:\n",
    "            self.df.drop(['diff'], axis=1, inplace=True)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def apply_detection_outliers(\n",
    "        self,\n",
    "        field_outliers: Dict[str, str]\n",
    "    ):\n",
    "        \n",
    "        self.detect_outliers_diff_fields(keep_column=True)\n",
    "        \n",
    "        # For the optional fields we want to compute\n",
    "        for key, value in field_outliers.items():\n",
    "            self.detect_outliers_field(key, value)\n",
    "        \n",
    "        return self.df\n",
    "        \n",
    "    \n",
    "    def get_periodical_data(\n",
    "        self,\n",
    "        column: str,\n",
    "        prefix: str\n",
    "    ):\n",
    "        \n",
    "        self.df[prefix + '_days'] = self.df[column].dt.day\n",
    "        self.df[prefix + '_month'] = self.df[column].dt.month\n",
    "        self.df[prefix + '_year'] = self.df[column].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "proprietary-leader",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_processing(data):\n",
    "    \"\"\"\n",
    "    Function to implement any type of post-processing required.\n",
    "    \"\"\"\n",
    "    \n",
    "    post_processing_instance = PostProcessing(data)\n",
    "    \n",
    "    # Get the periodical data needed\n",
    "    post_processing_instance.get_periodical_data('tpep_pickup_datetime', 'pickup')\n",
    "    post_processing_instance.get_periodical_data('tpep_dropoff_datetime', 'dropoff')\n",
    "\n",
    "    # Handle outlier information\n",
    "    field_outliers = {\n",
    "        'trip_distance': 'trip_distance_outlier',\n",
    "        'total_amount': 'total_amount_outlier',\n",
    "        'fare_amount': 'fare_amount_outlier'\n",
    "    }\n",
    "    \n",
    "    df = post_processing_instance.apply_detection_outliers(field_outliers)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-retreat",
   "metadata": {},
   "source": [
    "Create a new dataset that contains all the information for the years: 2019, 2020, and 2021.\n",
    "\n",
    "Remember that in order to reduce the memory required, you can take a subsample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hearing-melbourne",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e328aca5b5584204a3d9787c806ffaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7697 7696617\n",
      "7049 7049370\n",
      "7867 7866620\n",
      "7476 7475949\n",
      "7598 7598445\n",
      "6972 6971560\n",
      "6310 6310419\n",
      "6073 6073357\n",
      "6568 6567788\n",
      "7214 7213891\n",
      "6878 6878111\n",
      "6896 6896317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6405 6405008\n",
      "6299 6299367\n",
      "3008 3007687\n",
      "238 238073\n",
      "348 348415\n",
      "550 549797\n",
      "800 800412\n",
      "1007 1007286\n",
      "1341 1341017\n",
      "1681 1681132\n",
      "1509 1509000\n",
      "1462 1461898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370 1369769\n",
      "1372 1371709\n",
      "1925 1925152\n",
      "2171 2171187\n",
      "2507 2507109\n",
      "2834 2834264\n",
      "2822 2821746\n",
      "2789 2788757\n",
      "2964 2963793\n",
      "3464 3463504\n",
      "3473 3472949\n",
      "3214 3214369\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([clean_data(load_table(year, month), year, month) for year in tqdm(YEARS) for month in tqdm(range(1, 13), leave = False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = post_processing(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-interim",
   "metadata": {},
   "source": [
    "## 02. Visualizations (by years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-title",
   "metadata": {},
   "source": [
    "### Number of trips by year\n",
    "\n",
    "Can you answer the question: **Has covid increased / decreased the number of trips made by taxis?**\n",
    "\n",
    "To answer this question, create a bar figure showing the number of trips per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(df, column, xlabel, ylabel, title):\n",
    "    \"\"\"\n",
    "    A function that creates a bar figure from the dataframe *df* and the content of the *column* that contains the information.\n",
    "    \"\"\"\n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-bachelor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bar_plot(df, 'year', 'Any', 'Number of trips', 'Number of trips by year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-marsh",
   "metadata": {},
   "source": [
    "**Question: Is this the behavior you expected? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-entertainment",
   "metadata": {},
   "source": [
    "> ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-survey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "passive-administrator",
   "metadata": {},
   "source": [
    "Now, you will visualize **how many passengers there are per taxi and per year**.\n",
    "\n",
    "Create a figure with three subplots (one per year) where can be seen the number of passengers per year.\n",
    "\n",
    "Then repeat the same chart viewing the % (use the *norm* parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def passengers_taxi_year(df, ylim, xlabel, ylabel, title, norm = False):\n",
    "    \"\"\"\n",
    "    Function that displays how many passengers there are per taxi and per year\n",
    "    \"\"\"\n",
    "    \n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-wisdom",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "passengers_taxi_year(df, (0, 60000), 'Passengers per taxi', 'Count', 'Passengers per taxi and per year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-psychology",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "passengers_taxi_year(df, (0, 0.8), 'Passengers per taxi', '%', 'Percentage of passengers per taxi and per year', norm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-modeling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "developmental-glossary",
   "metadata": {},
   "source": [
    "In the previous figure, you have visualized each year separately. To make the visualization easier to interpret, combine all the information into a graph.\n",
    "\n",
    "The expected visualization has to contain three columns (different colors) for each number of passengers.\n",
    "\n",
    "Then repeat the same chart viewing the % (use the *norm* parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def passengers_taxi(df, xlabel, ylabel, norm = False):\n",
    "    \"\"\"\n",
    "    Function that displays how many passengers there are per taxi\n",
    "    \"\"\"\n",
    "        \n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-florida",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "passengers_taxi(df, 'Passengers per taxi', 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-picnic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "passengers_taxi(df, 'Passengers per taxi', '%', norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-stability",
   "metadata": {},
   "source": [
    "**Question: What impact have you seen on the data? Do you think covid had a lot of impact?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-nickname",
   "metadata": {},
   "source": [
    "> ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-rotation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "continental-catalyst",
   "metadata": {},
   "source": [
    "## 03. Number of trips\n",
    "\n",
    "So far, you have seen the number of trips there have been in the years studied.\n",
    "\n",
    "Let's study what changes can be seen if you aggregate the data by hours, days of the week, week of the year, and months.\n",
    "\n",
    "\n",
    "These visualizations have to be done for the *pick-up* and *drop-off* columns. \n",
    "\n",
    "Furthermore, the information has to be split by year and represented with dashed lines, and marked with a round or cross wherever the value is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trips(df, columns, title, xlabel, ylabel):\n",
    "    \"\"\"\n",
    "    Function that visualizes the number of trips by different data aggregations\n",
    "    \"\"\"\n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-chapter",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_trips(df, ['pickup_month', 'dropoff_month'], title = 'Number of trips per month', xlabel = 'Month of the year', ylabel = 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-affair",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_trips(df, ['pickup_week', 'dropoff_week'], title = 'Number of trips per week of the year', xlabel = 'Week of the year', ylabel = 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "velvet-heart",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize_trips' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [157], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_trips\u001b[49m(df, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_hour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropoff_hour\u001b[39m\u001b[38;5;124m'\u001b[39m], title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of trips per hour\u001b[39m\u001b[38;5;124m'\u001b[39m, xlabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime of day\u001b[39m\u001b[38;5;124m'\u001b[39m, ylabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'visualize_trips' is not defined"
     ]
    }
   ],
   "source": [
    "visualize_trips(df, ['pickup_hour', 'dropoff_hour'], title = 'Number of trips per hour', xlabel = 'Time of day', ylabel = 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-monitoring",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_trips(df, ['pickup_day', 'dropoff_day'], title = 'Number of trips per day of the week', xlabel = 'Day of the week', ylabel = 'Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-facing",
   "metadata": {},
   "source": [
    "**Question: What behaviors do you see in each case? What do you think is the reason?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-animal",
   "metadata": {},
   "source": [
    "> ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-compression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sticky-chancellor",
   "metadata": {},
   "source": [
    "## 04. Distance/duration and speed relationship\n",
    "\n",
    "In the data, there is the distance traveled by taxis on each trip. Furthermore, you can extract the duration of the trips using: *tpep_dropoff_datetime* and *tpep_pickup_datetime*.\n",
    "\n",
    "Now, you will find out how covid affected the distances and durations of journeys along with the speed of taxis.\n",
    "\n",
    "Do you think the traffic density changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-associate",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Visualize the **histograms** of distance and duration per year.\n",
    "\n",
    "You can use *plt.hist()* or *plt.bar()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_histograms(df, column, title, xlabel, ylabel, xlim):\n",
    "    \"\"\"\n",
    "    Function that creates a histogram from the information contained in the column *column* of the dataframe *df*\n",
    "    \"\"\"\n",
    "    \n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-southeast",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_histograms(df, 'trip_distance', title = 'Distance traveled per year', \n",
    "                     xlabel = 'Distance (km)', ylabel = 'Count', xlim = (-5, 80))\n",
    "\n",
    "visualize_histograms(df, 'trip_duration', title = 'Duration of trips per year', \n",
    "                     xlabel = 'Duration (h)', ylabel = 'Count', xlim = (-1, 25) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-champion",
   "metadata": {},
   "source": [
    "**QUESTIONS:**\n",
    "\n",
    "* How do you think covid affected travel distances and durations?\n",
    "\n",
    "* And the speed of taxis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-mauritius",
   "metadata": {},
   "source": [
    "> ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-expense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "related-identification",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scatter plot and correlation\n",
    "\n",
    "Create scatter plots to illustrate the relationship between trip duration and distance.\n",
    "\n",
    "It is possible that the data contain samples outside the distribution (outliers). In this case, skip this samples and display the figure again.\n",
    "\n",
    "To see if any correlation exists, it is interesting to use the *sns.regplot()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(df, x_value, y_value, xlabel, ylabel, remove_outliers = False):\n",
    "    \"\"\"\n",
    "    Function that displays a scatter plot given the name of the columns that contains the information\n",
    "    \"\"\"\n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-beatles",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scatter_plot(df, 'trip_distance', 'trip_duration', 'Distance (km)', 'Duration (h)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-certificate",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scatter_plot(df, 'trip_distance', 'trip_duration', 'Distance (km)', 'Duration (h)', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-genius",
   "metadata": {},
   "source": [
    "**Question: Can you see any relationship? Can you calculate the correlation between the data to get more information?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-remove",
   "metadata": {},
   "source": [
    "> ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-result",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pleased-flush",
   "metadata": {},
   "source": [
    "As you did in section 3, visualize the time and distance data for the weeks and months of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_per_period(df, column_data, columns, xlabel, ylabel, title):\n",
    "    \"\"\"\n",
    "    Function that show the distance / duration of trips in the time determined\n",
    "    \"\"\"\n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-latvia",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_per_period(df, 'trip_distance', columns = ['pickup_week', 'dropoff_week'],\n",
    "                    xlabel = 'Week of the year', ylabel = 'Mean distance (km)', title = 'Distance by weeks of the year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-negative",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_per_period(df, 'trip_distance', columns = ['pickup_month', 'dropoff_month'],\n",
    "                     xlabel = 'Month of the year', ylabel = 'Mean distance (km)', title = 'Distance by months of the year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-norway",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_per_period(df, 'trip_duration', columns = ['pickup_week', 'dropoff_week'],\n",
    "                     xlabel = 'Week of the year', ylabel = 'Mean duration (h)', title = 'Duration by weeks of the year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-triple",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_per_period(df, 'trip_duration', columns = ['pickup_month', 'dropoff_month'],\n",
    "                     xlabel = 'Month of the year', ylabel = 'Mean duration (h)', title = 'Duration by months of the year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-roberts",
   "metadata": {},
   "source": [
    "**Question: Is there any strange behavior apart from covid? What can it be caused by?**\n",
    "\n",
    "> ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-contract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "neural-warning",
   "metadata": {},
   "source": [
    "So far, you have shown the data by aggregating different information.\n",
    "\n",
    "Now, you have to visualize the data like images. For that, you will use the *plt.imshow()* function which displays images and arrays.\n",
    "\n",
    "Implement a function that displays heatmaps by year (each function display 3 heatmaps, one per year):\n",
    "\n",
    "- a heatmap showing what time of day are the longest trips during the year.\n",
    "- a heatmap showing what time of day are the longest trips during the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(df, group, column_data, xlabel, ylabel, columns = None):\n",
    "    \"\"\"\n",
    "    Function that aggregates data appropriately to display a heatmap\n",
    "    \"\"\"\n",
    "    \n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-badge",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heatmap(df, ['pickup_hour', 'pickup_dayofyear'], 'trip_duration', 'Days of the year', 'Hours of the day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-protest",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heatmap(df, ['pickup_hour', 'pickup_day'], 'trip_duration', 'Times of day', 'Days of the week', ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-cache",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "actual-participation",
   "metadata": {
    "tags": []
   },
   "source": [
    "Repeat the previous heatmaps visualizing the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(df, ['pickup_hour', 'pickup_dayofyear'], 'trip_distance', 'Days of the year', 'Times of the day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-plastic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heatmap(df, ['pickup_hour', 'pickup_day'], 'trip_distance', 'Times of the day', 'Days of the week', ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-library",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "federal-finger",
   "metadata": {},
   "source": [
    "Finally, view the average speed at different times of the day during the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-allah",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def speed_heatmap(df, group, xlabel, ylabel, columns = None):\n",
    "    \"\"\"\n",
    "    Function that aggregates data appropriately to display a speed heatmap\n",
    "    \"\"\"\n",
    "    \n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-sydney",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speed_heatmap(df, ['pickup_hour', 'pickup_day'], 'Times of the day', 'Days of the week', ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-delaware",
   "metadata": {},
   "source": [
    "**Question: Which conclusions do you obtain from the heatmaps?**\n",
    "    \n",
    "> ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-olive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "governing-segment",
   "metadata": {},
   "source": [
    "## 05. Visualize the locations of the trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-impression",
   "metadata": {},
   "source": [
    "The data only has the ID of a location, so you need to add the latitude and longitude.\n",
    "\n",
    "This information is saved in *data/geodata/taxi_zones.shp*.\n",
    "\n",
    "The next cells can be understood as a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_lat_lon, draw_region_map, draw_zone_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-rebecca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf = shapefile.Reader('data/geodata/taxi_zones.shp')\n",
    "\n",
    "fields_name = [field[0] for field in sf.fields[1:]]\n",
    "shp_dic = dict(zip(fields_name, list(range(len(fields_name)))))\n",
    "attributes = sf.records()\n",
    "shp_attr = [dict(zip(fields_name, attr)) for attr in attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc = pd.DataFrame(shp_attr).join(get_lat_lon(sf, shp_dic).set_index(\"LocationID\"), on=\"LocationID\")\n",
    "df_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-debut",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,8))\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.set_title(\"Boroughs in NYC\")\n",
    "draw_region_map(ax, sf, shp_dic)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.set_title(\"Zones in NYC\")\n",
    "draw_zone_map(ax, sf, shp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-ambassador",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "chemical-portsmouth",
   "metadata": {},
   "source": [
    "Now you have two dataframes that you need to join. Use the *pd.merge* function to add the longitude and latitude to the *df* dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE YOUR CODE\n",
    "merge = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-cooper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "roman-verse",
   "metadata": {},
   "source": [
    "## 06. Which are the areas/zones with more pick-up and drop-off?\n",
    "\n",
    "In this section, you have to visualize the areas where taxis are most used.\n",
    "\n",
    "The first step is to sort and save in a variable the most common places in the pick-up and drop-off.\n",
    "\n",
    "The variables *top_pu* and *top_do* contains a dataframe with columns: 'year', 'PULocationID', and 'count'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick-up\n",
    "# HERE YOUR CODE\n",
    "top_pu = ...\n",
    "top_pu.columns = ['year', 'PULocationID', 'count']\n",
    "\n",
    "# Drop-off\n",
    "# HERE YOUR CODE\n",
    "top_do = ...\n",
    "top_do.columns = ['year', 'DOLocationID', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-moisture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-vacation",
   "metadata": {},
   "source": [
    "Print the 5 most frequent zones per year and in each case (pick-up and drop-off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 5\n",
    "def show_top_n(df, column, df_loc, n_top = n_top):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that print the most common zones by year\n",
    "    \"\"\"\n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top_n(top_pu, 'PULocationID', df_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top_n(top_do, 'DOLocationID', df_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-universe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "plain-evanescence",
   "metadata": {},
   "source": [
    "**Let's see with a heat map which are the most common zones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS:\n",
    "    \n",
    "    PUcount = {k:v for k, v in top_do[top_do.year == year][['DOLocationID', 'count']].values}\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    ax.set_title(f\"Zones with most pickups - {year}\")\n",
    "    draw_zone_map(ax, sf, shp_dic, heat=PUcount, text=list(PUcount.keys())[:3])\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    ax.set_title(f\"Zones with most drop-offs - {year}\")\n",
    "    draw_zone_map(ax, sf, shp_dic, heat=PUcount, text=list(PUcount.keys())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-universal",
   "metadata": {},
   "source": [
    "**Question: Why do you think the Manhattan area has more trips?**\n",
    "\n",
    "> ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-destination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "protective-mambo",
   "metadata": {},
   "source": [
    "## 07. Hospitals\n",
    "\n",
    "How has the travel destination changed? Are more people going to hospitals?\n",
    "\n",
    "In the next cell, there is a DataFrame with the most important hospitals in New York and their locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals = [('New Yorks Presbyterian Lower Manhattan Hospital, NYC, NY, USA', '40.710255', '-74.005058'),\n",
    "('Manhattan Gastroenterology, NYC, NY, USA', '40.778259', '-73.958092'),\n",
    "('Stem Cell Therapy Hospital, NYC, NY, USA', '40.601517', '-73.961067'),\n",
    "('Park Avenue Smiles, Yonkers, NYC, NY, USA', '40.945873', '-73.890671'),\n",
    "('Cosmetic Dentistry Center, NYC, NY, USA', '40.629234', '-74.026077'),\n",
    "('Envy Smile Dental Spa, Brooklyn, NYC, NY, USA', '40.607059', '-73.960144'),\n",
    "('VIVA EVE, Forest Hills, NYC, NY, USA', '40.734291', '-73.849434'),\n",
    "('Forest Hills Medical Services, Queens, NYC, NY, USA', '40.734310', '-73.849510'),\n",
    "('Professional Gynecological Services, Brooklyn, NY, NY, USA', '40.689747', '-73.982346'),\n",
    "('Manhattan Womens Health & Wellness, New York, NY, USA', '40.741997', '-73.986107'),\n",
    "('Brooklyn Abortion Clinic, Brooklyn, NY, New York, USA', '40.689743', '-73.982368'),\n",
    "('Brooklyn GYN Place, Brooklyn, NY, USA', '40.692696', '-73.993584'),\n",
    "('Americas Holistic Doctor, NYC, NY, USA', '40.742531', '-73.985489'),\n",
    "('NJS Physical Medicine & Rehabilitation, Brooklyn, NY, USA', '40.641621', '-73.956734'),\n",
    "('DHD Medical, Brooklyn New York, USA', '40.625568', '-73.918320'),\n",
    "('Workers Compensation Doctor, New York, NY, USA', '40.652225', '-74.006104'),]\n",
    "\n",
    "hospitals = pd.DataFrame(hospitals, columns = ['Place Name', 'Latitude', 'Longitude'])\n",
    "hospitals['Latitude'] = hospitals['Latitude'].astype(float)\n",
    "hospitals['Longitude'] = hospitals['Longitude'].astype(float)\n",
    "hospitals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-duplicate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "necessary-rhythm",
   "metadata": {},
   "source": [
    "Use the latitude and longitude of each hospital and the latitude and longitude of the zone (information in *merge* dataframe) to see the distribution of the hospitals \"on the map\".\n",
    "\n",
    "To do this, use a scatter plot. It will be better understood if the points are transparent (parameter *alpha*).\n",
    "\n",
    "Also, remember how the longitude and latitude data have to be plotted in the figure.\n",
    "\n",
    "Then repeat the same scatter plot but separated by years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_locations(merge, hospitals):\n",
    "    \"\"\"Function showing the distribution of hospitals and taxi destinations\"\"\"\n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_locations(merge, hospitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_locations_per_year(merge, hospitals):\n",
    "    \"\"\"Function showing the distribution of hospitals and taxi destinations per year\"\"\"\n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_locations_per_year(merge, hospitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-skirt",
   "metadata": {},
   "source": [
    "**Question: Do you see any behavior?**\n",
    "\n",
    ">ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-sapphire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tired-species",
   "metadata": {},
   "source": [
    "To find out how trips to hospitals have changed, you need to detect in which zone is each hospital.\n",
    "\n",
    "To do it, you need to calculate the distances between the hospitals and the zones. \n",
    "\n",
    "Do not calculate the distance point-to-point, but matrix-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_loc = hospitals[['Longitude', 'Latitude']].values\n",
    "loc = df_loc[['longitude', 'latitude']].values\n",
    "\n",
    "# HERE YOUR CODE\n",
    "dist = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances as an image\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-cross",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "residential-swiss",
   "metadata": {},
   "source": [
    "Find a way to locate the nearest sector based on distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE YOUR CODE\n",
    "hospitals_locations = ...\n",
    "hospitals['LocationID'] = hospitals_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-atlas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hourly-beads",
   "metadata": {},
   "source": [
    "Now that you know the zone of each hospital, visualize how many trips are related to them.\n",
    "\n",
    "You will assume that all trips in the zone go to the hospital.\n",
    "\n",
    "Make a single figure with two bar plots showing the number of trips for each year and the % of total trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trips_hospitals(df, hospitals_locations):\n",
    "    \"\"\"\n",
    "    Function that displays a single figure with two bar plots showing the number of trips for each year and the % of total trips.\n",
    "    \"\"\"\n",
    "    # HERE YOUR CODE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_hospitals(df, hospitals_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-antibody",
   "metadata": {},
   "source": [
    "## 08. Curiosity killed the cat\n",
    "\n",
    "In the **first part** you must view and comment on the examples you have removed, such as very long distances, very short distances...\n",
    "\n",
    "In the **second part** you are free to choose and make visualizations that bring you extra information that has not been seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-escape",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "excellent-halloween",
   "metadata": {},
   "source": [
    "## 09. Report\n",
    "\n",
    "Based on the data exploration you have done throughout this notebook, make a short report summarizing and justifying all the changes that have arisen due to covid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-inclusion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
